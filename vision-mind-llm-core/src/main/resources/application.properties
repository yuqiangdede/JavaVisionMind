spring.application.name=llm
server.port=17004

server.servlet.context-path=/vision-mind-llm
# Configure either Ollama or OpenAI endpoints; OpenAI takes precedence when both are set
#ollama.base-url=http://127.0.0.1:11434
#ollama.chat.options.model=gemma3:1b
ollama.base-url=
ollama.chat.options.model=

openai.base-url=http://127.0.0.1:8080/v1/chat/completions
openai.api-key=1234
openai.chat.options.model=gemma-3-4b-it-q4-k-m
#openai.base-url=ark.cn-beijing.volces.com/api/v3/chat/completions
#openai.api-key=YOUR_API_KEY
#openai.chat.options.model=ep-20250327212217-f57wv
